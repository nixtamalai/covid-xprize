{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nixtamal AI Pandemic Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Quantitative Evaluation Criteria\n",
    "\n",
    "Teams must copy all necessary models and data into their evaluation sandbox and verify that it can be executed per the previously mentioned command syntax by December 22. The leaderboard will report the predictor results regularly. The predictor submissions will be evaluated on all regions and\n",
    "then separately on the speciality regions. Judges may also cumulatively evaluate submissions on larger regions, such as specific countries, continents, and the world.\n",
    "\n",
    "At the conclusion of the Phase One Live Model Testing, the data from submissions will be **ranked in each region** according to the cumulative error in the 7-day moving average for the number of cases per 100,000 people.\n",
    "Based on such region-specific rankings, two overall performance measures will be formed. These are the:\n",
    "* Mean ranking of teams across all regions\n",
    "* Mean ranking of teams across the specialty regions, if selected\n",
    "\n",
    "In their predictions, teams are encouraged to produce interesting results and show them. Judges will also consider any additional quantitative data that teams can provide. For example, whether predictor outputs include optional fields, such as **confidence intervals, death rates, hospitalization rates, ventilators needed, and other outputs**. Since death rates are available in Oxford University’s Blavatnik School of Government’s data, and they can be predicted like cases, teams may find it helpful to predict death rates as well as additional sources of information. If enough teams do this, the accuracy of **death-rate predictions may be measured similarly to cases**. And the rankings can be presented to the judges as optional quantitative information.\n",
    "\n",
    "As a reminder to teams, and as explained in the GitHub repository, only a subset of the NPIs found in the Oxford dataset will be used in the evaluation of the predictor models. Specifically, only the NPIs that have a direct impact on the spread of the virus (i.e. on the daily new cases number) and are identified as \"Containment and health index\" in the Oxford Index Methodology document (found here) will be used. These NPIs are C1 to C8 (inclusive) and H1, H2, H3, and H6. Please refer to the sample code found on the GitHub repository for more examples of these indices and how they are implemented in the predictor submission template.\n",
    "Teams will have their work subject to the following sanity checks:\n",
    "* Ranking on retrospective runs on historical intervals, representing a broader range of situations than encountered in live testing\n",
    "* Other predictor sanity check pass/fail results (e.g., negative predictions, maximal and minimal stringency predictions, and predictions exceeding population size)\n",
    "\n",
    "The quantitative evaluation will be used by the Judging Panel to eliminate the most inaccurate models before the qualitative evaluation phase. **Up to 100 teams may be eliminated in this phase**, but the exact number will depend on the total number of registered teams and the distribution of accuracy among the all submitted models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation Criteria\n",
    "After the preliminary quantitative evaluation phase described above, the Judging Panel will use a combination of both quantitative and qualitative evaluation criteria to evaluate the teams. The\n",
    "  \n",
    "qualitative judging of the teams’ models will begin during the predictor evaluation period. For the qualitative evaluation, the Judging Panel will use the following criteria with a generally equal weighting:\n",
    "* Innovation​: Teams who submit and use additional data, intervention plans (such as vaccination policies and treatments), or otherwise find innovative ways to extend the scope of the challenge will be ranked highly;\n",
    "* Generality​: Teams will first be evaluated on how well their models perform across all regions. Subsequently, teams will be awarded bonus points for how well their models do in specialty regions;\n",
    "* Collaborative contributions​: Teams that take an open-source approach to the data or models that they use, and who contribute data and models to the shared success of all teams will be ranked highly;\n",
    "* Consistency​: Approaches that stay within an acceptable range of accuracy in the short and long term, and that perform as expected in any scenario analyses run by the Judging Panel, are preferred;\n",
    "* Speed and resource use​: Model that are faster and more efficient in their approach are preferred;\n",
    "* Addressing the challenge​: Teams must avoid taking shortcuts or finding loopholes to improve their quantitative performance at the expense of real-world performance. Additionally, teams may be awarded bonus points for predicting additional, relevant public health metrics such as required hospital beds and ventilators; and\n",
    "* Explanation​: Submissions should include a narrative description of how the model works, the data it uses, and its sources as well as any relevant points related to these themes. Furthermore, models that emphasize interpretability by being able to explain why the model is predicting what it does (i.e. glass-box models) will be ranked highly.\n",
    "\n",
    "At the conclusion of Phase One—Predictor Judging, each teams’ qualitative and quantitative scores will be combined and teams will be ranked by the Judging Panel. Up to 50 teams will advance to Phase Two—Prescriptors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data used to run the model;\n",
    "\n",
    "Describir datos extra que usamos con citas de fuentes. Aquí podríamos compartir datos sobre México para que se incluyan en los datos de Oxfram. Que los equipos aporten datos a la comunidad también se va a tomar en cuenta por los jueces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "* A description of the approach taken in developing the model which should also address innovation, generality, collaboration, and other qualitative judging criteria (submitted via POP); and\n",
    "* Optionally, teams can highlight the list of “specialty regions” they would like judges to consider for their model. These regions are the focus of a team’s predictor model beyond the general evaluation. In these regions, their performance will be measured and judged separately. (**Note**: please refer to the Oxford dataset for a list of all available regions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Descripción de la salida del modelo. Mínimo debe seguir los lineamientos de la guía:\n",
    "\n",
    "* One row per day per region for which an intervention plan was supplied;\n",
    "* Required Columns: Date, CountryName, RegionName, PredictedDailyNewCases; and\n",
    "* Optional Columns: Teams may produce additional columns as output of their predictor\n",
    "models in the CSV file. These columns will be noted by the judges but not evaluated by the Robo Judge. Example optional columns could be:\n",
    "* A column labeled IsSpecialty to indicate whether a region is to be considered a speciality region for your model (1 = speciality region, 0 = not a speciality region)\n",
    "* A 95% confidence interval and standard deviation for predicted number of cases\n",
    "* Predicted number of deaths and related 95% confidence intervals\n",
    "* Predicted number of hospitalization rates and related 95% confidence intervals\n",
    "* Predicted number of ventilators needed and related 95% confidence internals\n",
    "* Other columns chosen by the team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "197.667px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
